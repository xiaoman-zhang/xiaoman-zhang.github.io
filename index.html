
<!DYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiaoman Zhang</title>
  
  <meta name="author" content="Xiaoman Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name> Xiaoman Zhang (张小嫚) </name>
              </p>
              <p> Hi, I am a PhD student at <a href="https://mediabrain.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, advised by <a href="https://weidixie.github.io/">Prof. Weidi Xie</a> and <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Prof. Ya Zhang</a>.
		            I received my bachelor degree from <a href='https://www.ustc.edu.cn/'> University of Science and Technology of China (USTC)</a> in June 2019. 
              </p>
              <p> 
                My primary research interest is in the development of innovative intelligent systems that can match the proficiency of top physicians, supporting higher-quality medical diagnosis, intervention, and treatment. 
        	</p> 
              <p> <strong>How to proficiently collect extensive multi-modal data on a large scale, adeptly model these intricate data, and implicitly integrate medical domain-specific knowledge during modeling</strong> are the central questions that drive my research.
              </p>            
              <p> I will be at ICCV 2023! Let me know if you want to chat! </p>         
              <p style="text-align:center">
                <a href="xm99sjtu@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Zno4WggAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xiaoman-zhang/">Github</a> &nbsp/&nbsp
                <a href="./files/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://twitter.com/XiaomanZhang99">Twitter</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="image/zxm.png"><img style="width:100%;max-width:100%" alt="profile photo" src="image/zxm.png" class="hoverZoomLink"></a>
            </td>
          </tr>
	</tbody></table>
	<br />
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading><b></bold>Research</b></heading>
        
        <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/RadFM.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://chaoyi-wu.github.io/RadFM/">
                <papertitle>Towards Generalist Foundation Model for Radiology</papertitle>
              </a>
              <br>
              <a href="https://chaoyi-wu.github.io/">Chaoyi Wu*</a>,
              <strong>Xiaoman Zhang*</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em>Technical Report, 2023.</em><br>
              In this paper, we have constructed a complete set of medical foundation model-building processes, including data collection, problem formulation, model design, training, and evaluation. We construct the largest medical multi-modal database in this paper and in model capabilities, compared to existing work, our model is able to process multiple 3D or 2D image inputs interleaved with texts, which fits the practical usage more. We surpass the latest open-source multi-modal foundation model significantly.
            </td>
          </tr>

        <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/PMC-VQA.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://xiaoman-zhang.github.io/PMC-VQA">
                <papertitle>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</papertitle>
              </a>
              <br>
              <strong>Xiaoman Zhang*</strong>,
              <a href="https://chaoyi-wu.github.io/">Chaoyi Wu*</a>,
              <a>Ziheng Zhao</a>,
	      <a>Weixiong Lin</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em>Technical Report, 2023.</em><br>
              In this paper, we introduce PMC-VQA, a large-scale medical visual question-answering dataset, which contains 227k VQA pairs of 149k images that cover various modalities or diseases.
            </td>
          </tr>

	  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/PMC-LLaMA.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.14454v1.pdf">
                <papertitle>PMC-LLaMA: Further Finetuning LLaMA on Medical Papers</papertitle>
              </a>
              <br>
              <a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a>,
              <strong>Xiaoman Zhang</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em>Technical Report, 2023.</em><br>
              In this report, we introduce PMC-LLaMA, an open-source language model that is acquired by fine-tuning an open-source language model on a total of 4.8 million biomedical academic papers for further injecting medical knowledge, enhancing its capability in medical domain.
            </td>
          </tr>
	  
	      
	   <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:25%;vertical-align:middle">
              <img src="image/PMC_CLIP.png" width="320" height="100" style="border-style: none">
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.07240">
                <papertitle>PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</papertitle>
              </a>
              <br>
              <a>Weixiong Lin*</a>,
              <a>Ziheng Zhao*</a>,
              <strong>Xiaoman Zhang</strong>,
	      <a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em>MICCAI</em>, 2023 <br>
              We collect a biomedical dataset, PMC-OA with <strong>1.6M</strong> image-caption pairs collected from PubMedCentral's OpenAccess subset.
            </td>
          </tr>
	  
	   <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:40%;vertical-align:middle">
              <img src="image/KAD.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="https://xiaoman-zhang.github.io/KAD/">
                <papertitle>Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images
              </a>
              <br>
              <strong>Xiaoman Zhang</strong>,
	      <a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em> <strong> Nature Communications</strong> </em>, 2023.  (Impact Factor: ~18) <br> 
              In this paper, we propose a knowledge-enhanced vision-language pre-training approach for auto-diagnosis on chest X-ray images. The algorithm, named Knowledge-enhanced Auto Diagnosis (KAD), first trains a knowledge encoder based on an existing medical knowledge graph, and then leverages the pre-trained knowledge encoder to guide the visual representation learning with paired chest X-rays and radiology reports. 
            </td>
	  </tr> 
	      
	   <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:40%;vertical-align:middle">
              <img src="image/K-Diag.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="https://chaoyi-wu.github.io/K-Diag/">
                <papertitle>K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging</papertitle>
              </a>
              <br>
              <a href="https://chaoyi-wu.github.io/">Chaoyi Wu*</a>,
              <strong>Xiaoman Zhang*</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em>MICCAI Big Task Small Data Workshop</em>, 2023 &nbsp <font color="black"><strong>(Oral)</strong></font><br> 
              In this paper, we consider the problem of disease diagnosis. Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge.
            </td>
	  </tr> 
	     
	  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:40%;vertical-align:middle">
              <img src="image/MedKLIP.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="https://chaoyi-wu.github.io/MedKLIP/">
                <papertitle>MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology</papertitle>
              </a>
              <br>
	      <a href="https://chaoyi-wu.github.io/">Chaoyi Wu</a>,
              <strong>Xiaoman Zhang</strong>,
              <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              <em>ICCV</em>, 2023   
	      <br>
              We propose to leverage medical specific knowledge enhancing language-image pre-training method, significantly advancing the ability of pre-trained models to handle unseen diseases on zero-shot classification and grounding tasks.
          </td>
	 </tr> 
	  
	  <tr onmouseout="ld_stop()" onmouseover="ld_start()">
            <td style="padding:0px;width:40%;vertical-align:middle">
              <img src="image/LD.png" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="https://xiaoman-zhang.github.io/Layer-Decomposition/">
                <papertitle>Self-supervised Tumor Segmentation with Sim2Real Adaptation</papertitle>
              </a>
              <br>
              <strong>Xiaoman Zhang</strong>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://scholar.google.com/citations?user=BAZSE7wAAAAJ&hl=zh-CN">Chaoqin Huang</a>,
	      <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>,
	      <a>Xin Chen</a>,
	      <a>Qi Tian</a>,
	      <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>
              <br>
	      <em> IEEE Journal of Biomedical and Health Informatics</em>, 2023   
	      <br>
              we propose a two-stage Sim2Real training regime for unsupervised tumor segmentation, 
		where we first pre-train a model with simulated tumors, 
		and then adopt a self-training strategy for downstream data adaptation. 
            </td>
	 </tr> 
		  
	  <tr onmouseout="sar_stop()" onmouseover="sar_start()">
            <td style="padding:0px;width:40%;vertical-align:middle">
              <img src="image/SAR.jpg" width="320" height="120" style="border-style: none">
            </td>
            <td style="padding:10px;width:60%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2010.06107/">
                <papertitle>SAR: Scale-Aware Restoration Learning for 3D Tumor Segmentation</papertitle>
              </a>
              <br>
              <strong>Xiaoman Zhang</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=Yo-RbKIAAAAJ">Shixiang Feng</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=9WvLlkIAAAAJ">Yuhang Zhou</a>,
	      <a href="https://mediabrain.sjtu.edu.cn/members/">Yanfeng Wang </a>,
              <a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a>
              <br>
							<em>MICCAI</em>, 2021
              <p></p>
            </td>
          </tr> 

	      
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
		Based on a template by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
<!--for page update--> 
