<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>Self-supervised Tumor Segmentation with Sim2Real Adaptation</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Self-supervised Tumor Segmentation with Sim2Real Adaptation</span><br><br><br>
	</center>
	<table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://scholar.google.com/citations?user=BAZSE7wAAAAJ&hl=fr">Chaoqin Huang</a><sup>1</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
	      <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
		</td>
                    <td align="center" width="160px">	
	      <center>
                <span style="font-size:16px">Xin Chen</a><sup>3</sup></span>
                </center>
		</td>
                    <td align="center" width="160px">	
	   </tr>
	   <table align="center" width="320px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px">Qi Tian</a><sup>3</sup></span>
                </center>
		</td>
                    <td align="center" width="160px">		    
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2</sup></span>
                </center>
            </tr>

        </tbody></table><br>
	
	  <table align="center" width="800px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="250px">
	      <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Laboratory</span>
                </center>
                </td>
                    <td align="center" width="200px">
              <center>
                    <span style="font-size:16px"><sup>3</sup>Huawei Cloud</span>
                </center>
                </td>
        </tr></tbody></table>
	
	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/xiaoman-zhang/Layer-Decomposition"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://ieeexplore.ieee.org/document/10032792"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./cite.txt"> [BibTeX]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
	
      <br><hr>
      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <left>
	This paper targets on self-supervised tumor segmentation. 
	We make the following contributions: 
	(i) we take inspiration from the observation that tumors are often characterised independently of their contexts, we propose a novel proxy task ``layer-decomposition", that closely matches the goal of the downstream task, and design a scalable pipeline for generating synthetic tumor data for pre-training; 
	(ii) we propose a two-stage Sim2Real training regime for unsupervised tumor segmentation, where we first pre-train a model with simulated tumors, and then adopt a self-training strategy for downstream data adaptation; 
	(iii) when evaluating on different tumor segmentation benchmarks,e.g., BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation, our approach achieves state-of-the-art segmentation performance under the unsupervised setting. While transferring the model for tumor segmentation under a low-annotation regime, the proposed approach also outperforms all existing self-supervised approaches;
	(iv) we conduct extensive ablation studies to analyse the critical components in data simulation, and validate the necessity of different proxy tasks. We demonstrate that, with sufficient texture randomization in simulation, model trained on synthetic data can effortlessly generalise to datasets with real tumors.
      </left></p>
      <p><img class="left"  src="./resources/framework.png" width="800px"></p>
	
      <br><hr>
      <center> <h2> Data Synthesis </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
	  Illustration of the data simulation pipeline.
	  (a) Data simulation pipeline. We first simulate the tumor with a transformed image and a generated 3D mask, which provide texture and shape respectively. Then the synthetic data is composed by blending the simulated tumor into the normal image.
	  (b) Visualisation of shape simulation. The first row presents the 3D polyhedrons with various shapes, sizes and locations, and the second row shows the binary masks.
	  (c) Visualisation of texture simulation. We have displayed a randomly selected normal image undergoing different transformations functions (top row), the corresponding simulated tumor (second row), and synthetic training data (bottom row) in columns 2-8. 
	</left></p>
        <p><img class="left"  src="./resources/supple_fig_datasyn.png" width="800px"></p>
      <br>
      <hr>

      <center><h2>Visualizations of Zero-shot Tumor Segmentation </h2></center>
      <p><b>2D Visualization </b> </p>
      <p><left>
	      From left to right: input volume, ground truth (green) vs. predicted mask (red), reconstructed normal organ, reconstructed tumor, predicted mask.
      </left></p>
      <p><img class="center"  src="./resources/result.png" width="800px"></p>
	
      <p><b>3D Visualization </b> </p>
      <p><left>
	      From left to right: input volume, ground truth, reconstructed normal organ, reconstructed tumor, predicted mask.
      </left></p>
      <p><img class="left"  src="./resources/result_bt/concat_30.gif" width="800px"></p>
      <p><img class="left"  src="./resources/result_bt/concat_5.gif" width="800px"></p>
      <p><img class="left"  src="./resources/result_lits/concat_4.gif" width="800px"></p>
      <p><img class="left"  src="./resources/result_lits/concat_8.gif" width="800px"></p>
      <br>
      <hr>

      <center><h2>Results</h2></center>
      <p><b>R1: Fully Supervised Fine-tuning </b> </p>
      <p><left>
	      Compare with SOTA self-supervised methods on WTS (brain whole tumor segmentation) and LTS (liver tumor segmentation).
      </left></p>
      <p><img class="center"  src="./resources/table_1.png" width="800px"></p>
      <p><left>
	      Compare with SOTA self-supervised methods on BTS (Whole tumor, Tumor core and Enhanced tumor segmentation).
      </left></p>
      <p><img class="center"  src="./resources/table_2.png" width="800px"></p>
	
      <p><b>R2: Analysis of Model Transferability </b> </p>	
	<div class="container">
		<div class="image">
			<img style="width:400px" src='./resources/result_2.png'></img>
		</div>
		<div class="text"> 
			<p> We study the usefulness of self-supervised learning by varying the number of available volume annotations. 
				For both tasks, our proposed method shows superior performance on all supervision level.  
			</p>
		</div>
	</div>
      <p><img class="center"  src="./resources/table_3.png" width="800px"></p>
	
      <p><b>R3: Results of Zero-shot Tumor Segmentation </b></p>
	<div class="container">
		<div class="image">
			<img style="width:400px" src='./resources/table.png'></img>
		</div>
		<div class="text"> 
			<p>  We compare with approaches that advocate zero-shot tumor segmentation, <i>i.e.</i> the state-of-the-art unsupervised anomaly segmentation methods.
				As shown in the Table, our approach surpasses the other ones by a large margin.
			</p>
		</div>
	</div>
      
      <br>
      <hr>
      <center> <h2> Acknowledgements </h2> </center>
      <p> 
	      Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
      </p>
      <br>


<br>
</body>
</html>
