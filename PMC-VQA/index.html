<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>
	<title>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</span><br><br><br>
	</center>
	<table align="center">
            <tbody><tr>
                    <td align="center" width="270px">
              <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang*</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="200px">
              <center>
                <span style="font-size:16px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu*</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="200px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/members/">Ziheng Zhao</a><sup>1</sup></span>
                </center>
                </td>
                <td align="center" width="200px">
                  <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/members/">Weixiong Lin</a><sup>1</sup></span>
                </center>
                </td>
              </tr>
            </tbody></table><br>
  <table align="center">
              <tbody><tr>
                    <td align="center" width="200px">
              <center>
		 <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="200px">
	      <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2</sup></span>
                </center>
		</td>
                    <td align="center" width="200px">	    
              <center>
		<span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2</sup></span>
                </center>
            </tr>

        </tbody></table><br>
	
	<table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Lab</span>
                </center>
                </td>
        </tr></tbody></table>
	
	<table align="center" width="800px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/xiaoman-zhang/PMC-VQA"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://github.com/xiaoman-zhang/PMC-VQA"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./cite.txt"> [BibTeX]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                    <br>
                    <span style="font-size:20px">
                    Dataset <a href="https://github.com/xiaoman-zhang/PMC-VQA">[ðŸ¤—]</a>
                    <!-- <a href="https://pan.baidu.com/s/1mD51oOYbIOqDJSeiPNaCCg">[Baidu]</a> <span style="font-size: 15px">(key: 3iqf)</span> -->
                    </span>
                </center>
              </td>

            </tr></tbody>
      </table>
	
      <br><hr>
      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <left>
        In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA), which is crucial in efficiently interpreting medical images carrying vital clinic-relevant information. Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction. Specifically, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model.
        Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, e.g., VQA-RAD and SLAKE, outperforming existing work by a large margin.
        Additionally, we propose a test set, which is significantly more challenging than all existing ones, even the best model struggles to solve.
      </left></p>
      
      <br><hr>
      <center> <h2> Architecture </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <p><img class="left"  src="./resources/PMC-VQA.png" width="800px"></p>
      <p><left>
        (a) The proposed architecture of MedVInt, mainly consists of three components: a visual encoder to extract visual features, a language encoder to encode textual context, and a multimodal decoder to generate the answer; 
        (b) The proposed question-answer pairs generation pipeline.
      </left></p>
	
	
      <br><hr>
      <center> <h2> The PMC-VQA Dataset </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <p><left>
        Several examples of challenging questions and answers along with their respective images. To answer questions related to these images, the network must acquire sufficient medical knowledge, for example, for the first two images, it is essential to recognize the anatomy structure and modalities; for the third image, recognizing the X-ray image pattern of pathologies is necessary; for the final two images, apart from the basic biomedical knowledge, the model is also required to discern colors, differentiate subfigures, and perform Optical Character Recognition (OCR) is required.
      </left></p>
      <p><img class="left"  src="./resources/example.png" width="800px"></p>
      <p><b>Analysis on Images </b> </p>
      <p><left>
        The top 20 figure types in PMC-VQA, cover a wide range of diagnostic procedures.
      </left></p>
      <p><img class="left"  src="./resources/image_distribution.png" width="800px"></p>
      <p><b>Analysis on Questions </b> </p>
      <p><left>
        Question distribution of the training set by their first four words. From left to right are all questions, questions started with ``What'' and questions started with ``Which''.
        The ordering of the words starts towards the center and radiates outwards.
        We found a surprising variety of question types, including "What is the difference...", "What type of imaging...", and "Which image shows...". Most questions range from 5 to 15 words.
      </left></p>
      <p><img class="left"  src="./resources/question_distribution.png" width="800px"></p>
      <p><b>Analysis on  Answers </b> </p>
      <p><left>
      Detailed information about the top 50 words that appeared in the answers.
      The words in answers primarily encompass positional descriptions such as left and right, image modality such as CT/MRI, and specific anatomical regions. 
      </left></p>
      <p><img class="left"  src="./resources/answer_distribution.png" width="800px"></p>


      <br><hr>
      <center><h2>Results</h2></center>
      <p><b>R1: VQA-RAD and SLAKE </b> </p>
      <p><left>
	      Comparison to SOTA approaches on VQA-RAD and SLAKE. We use the blank model for evaluation. 
        Pre-training data indicates whether the model is pre-trained on the medical multi-modal dataset before training on the target dataset. 
        The best result is in red, the second-best result is in blue
      </left></p>
      <center><p><img class="center"  src="./resources/table1.png" width="700px"></p></center>

      <p><b>R2: PMC-VQA Benckmark </b> </p>	
      <p><left>
        Comparison of baseline models using different pre-trained models on both open-ended and multiple-choice tasks. We reported the results on PMC-VQA-test / PMC-VQA-test-clean.
      </left></p>
      <center><p><img class="center"  src="./resources/table2.png" width="700px"></p></center>

      <p><b>R3: Qualitative Results </b> </p>	
      <center><p><img class="center"  src="./resources/sample.png" width="800px"></p></center>
<br>
</body>
</html>
